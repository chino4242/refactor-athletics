# Macro Logging Issue & Data Architecture Analysis

## üî¥ ISSUE: Macros Not Saving in Production

### Root Cause Analysis

**Current Flow:**
1. User opens MacroLogModal
2. Clicks LOG button ‚Üí calls `handleQuickLog()`
3. `handleQuickLog()` calls `onLog(type, val, mode)`
4. `onLog` in NutritionTracker calls `logHabit()` from api.ts
5. `logHabit()` makes client-side fetch to `/api/habit/log`
6. **PROBLEM**: This API route doesn't exist in the codebase

**Evidence:**
```tsx
// src/services/api.ts:66
export const logHabit = async (...) => {
    const response = await fetch('/api/habit/log', {  // ‚ùå Route not found
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ userId, habitId, value, bodyweight, label, timestamp })
    });
}
```

**Missing API Route:**
- No file at `src/app/api/habit/log/route.ts`
- Macros fail silently (no error handling in UI)
- User sees no feedback that save failed

---

## ‚úÖ IMMEDIATE FIX

### Option 1: Use Existing Server Action (Recommended)
Replace `logHabit()` calls with `logHabitAction()` which already exists and works.

**Change in NutritionTracker.tsx:**
```tsx
// OLD (broken):
await logHabit(userId, habitId, finalVal, userProfile.bodyweight, label);

// NEW (working):
await logHabitAction(userId, habitId, finalVal, userProfile.bodyweight, label);
```

**Why this works:**
- `logHabitAction` is a Next.js Server Action
- Directly inserts into Supabase
- Already used successfully in DailyQuest component
- Has proper error handling

---

### Option 2: Create Missing API Route
Create `src/app/api/habit/log/route.ts`:

```ts
import { createClient } from '@/utils/supabase/server';
import { NextResponse } from 'next/server';

export async function POST(request: Request) {
    const { userId, habitId, value, bodyweight, label, timestamp } = await request.json();
    const supabase = await createClient();
    
    const ts = timestamp || Math.floor(Date.now() / 1000);
    const dateStr = new Date(ts * 1000).toISOString().split('T')[0];
    const xp = habitId.includes('meal_prep') ? 100 : (habitId.includes('sleep') ? 15 : 10);

    const { data, error } = await supabase
        .from('history')
        .insert({
            user_id: userId,
            exercise_id: habitId,
            timestamp: ts,
            date: dateStr,
            raw_value: value,
            value: label || habitId,
            level: 1,
            xp: xp,
            rank_name: 'Novice'
        })
        .select()
        .single();

    if (error) {
        return NextResponse.json({ error: error.message }, { status: 500 });
    }

    return NextResponse.json({ xp_earned: xp });
}
```

---

## üìä CURRENT DATA ARCHITECTURE

### Single Table Design: `history`
**All data types stored in one table:**
- Workouts (bench press, squats)
- Habits (steps, water, sleep)
- Macros (protein, carbs, fat, calories)
- Body measurements (weight, waist, arms)

**Schema:**
```sql
CREATE TABLE public.history (
  id uuid PRIMARY KEY,
  user_id uuid REFERENCES users(id),
  exercise_id text,           -- "macro_protein", "habit_steps", "bench_press"
  timestamp bigint,
  date text,
  value text,                 -- Display value
  raw_value numeric,          -- Actual number
  rank_name text,
  level integer,
  xp integer,
  details jsonb,              -- For workout sets
  created_at timestamp
);
```

### Pros of Current Design:
‚úÖ Simple - one table to query  
‚úÖ Easy to calculate total XP  
‚úÖ Unified history timeline  
‚úÖ Works for most use cases  

### Cons of Current Design:
‚ùå **Inefficient queries** - filtering by exercise_id on every request  
‚ùå **No aggregation** - must sum macros client-side  
‚ùå **Duplicate data** - calories logged twice (once as macro, once auto-calculated)  
‚ùå **Poor indexing** - no composite indexes for common queries  
‚ùå **Mixed concerns** - workouts and nutrition in same table  

---

## üöÄ OPTIMIZATION RECOMMENDATIONS

### 1. Add Indexes (Quick Win - No Schema Change)
```sql
-- Speed up daily totals query
CREATE INDEX idx_history_user_date ON history(user_id, date);

-- Speed up exercise-specific queries
CREATE INDEX idx_history_user_exercise ON history(user_id, exercise_id);

-- Speed up time-range queries
CREATE INDEX idx_history_user_timestamp ON history(user_id, timestamp DESC);
```

**Impact:** 10-100x faster queries for daily totals

---

### 2. Create Materialized View for Daily Totals (Medium Effort)
```sql
CREATE MATERIALIZED VIEW daily_totals AS
SELECT 
    user_id,
    date,
    SUM(CASE WHEN exercise_id = 'macro_protein' THEN raw_value ELSE 0 END) as protein,
    SUM(CASE WHEN exercise_id = 'macro_carbs' THEN raw_value ELSE 0 END) as carbs,
    SUM(CASE WHEN exercise_id = 'macro_fat' THEN raw_value ELSE 0 END) as fat,
    SUM(CASE WHEN exercise_id = 'macro_calories' THEN raw_value ELSE 0 END) as calories,
    SUM(CASE WHEN exercise_id = 'habit_water' THEN raw_value ELSE 0 END) as water,
    SUM(CASE WHEN exercise_id = 'habit_steps' THEN raw_value ELSE 0 END) as steps,
    SUM(xp) as total_xp
FROM history
WHERE exercise_id IN ('macro_protein', 'macro_carbs', 'macro_fat', 'macro_calories', 'habit_water', 'habit_steps')
GROUP BY user_id, date;

-- Refresh on insert
CREATE OR REPLACE FUNCTION refresh_daily_totals()
RETURNS TRIGGER AS $$
BEGIN
    REFRESH MATERIALIZED VIEW CONCURRENTLY daily_totals;
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER trigger_refresh_daily_totals
AFTER INSERT ON history
FOR EACH STATEMENT
EXECUTE FUNCTION refresh_daily_totals();
```

**Impact:** Instant daily totals (no client-side aggregation)

---

### 3. Separate Tables by Domain (High Effort - Best Long-term)

#### New Schema:
```sql
-- Keep history for workouts only
CREATE TABLE workouts (
  id uuid PRIMARY KEY,
  user_id uuid REFERENCES users(id),
  exercise_id text REFERENCES catalog(id),
  timestamp bigint,
  date text,
  value text,
  raw_value numeric,
  rank_name text,
  level integer,
  xp integer,
  sets jsonb
);

-- New table for nutrition
CREATE TABLE nutrition_logs (
  id uuid PRIMARY KEY,
  user_id uuid REFERENCES users(id),
  date text,
  timestamp bigint,
  protein numeric DEFAULT 0,
  carbs numeric DEFAULT 0,
  fat numeric DEFAULT 0,
  calories numeric DEFAULT 0,
  water numeric DEFAULT 0,
  created_at timestamp
);

-- New table for habits
CREATE TABLE habit_logs (
  id uuid PRIMARY KEY,
  user_id uuid REFERENCES users(id),
  habit_id text,
  date text,
  timestamp bigint,
  value numeric,
  xp integer,
  created_at timestamp
);

-- New table for body measurements
CREATE TABLE body_measurements (
  id uuid PRIMARY KEY,
  user_id uuid REFERENCES users(id),
  date text,
  weight numeric,
  waist numeric,
  arms numeric,
  chest numeric,
  legs numeric,
  shoulders numeric,
  body_fat_percentage numeric,
  created_at timestamp
);
```

#### Benefits:
‚úÖ **Atomic updates** - update all macros for a day in one row  
‚úÖ **Efficient queries** - no filtering needed  
‚úÖ **Type safety** - numeric columns instead of jsonb  
‚úÖ **Better analytics** - easy to aggregate nutrition over time  
‚úÖ **Cleaner code** - domain-specific queries  

#### Migration Strategy:
1. Create new tables alongside existing `history`
2. Dual-write to both tables for 1 week
3. Backfill new tables from `history`
4. Switch reads to new tables
5. Deprecate `history` table (or keep for workouts only)

---

## üéØ RECOMMENDED ACTION PLAN

### Phase 1: Fix Macro Logging (Today)
1. ‚úÖ Replace `logHabit()` with `logHabitAction()` in NutritionTracker
2. ‚úÖ Add error handling and user feedback
3. ‚úÖ Test in production

### Phase 2: Performance (This Week)
1. Add indexes to `history` table
2. Monitor query performance with Supabase dashboard
3. Consider materialized view if queries still slow

### Phase 3: Architecture (Next Sprint)
1. Design new table schema
2. Create migration scripts
3. Implement dual-write pattern
4. Backfill historical data
5. Switch to new tables

---

## üìà EXPECTED IMPROVEMENTS

| Metric | Current | With Indexes | With New Tables |
|--------|---------|--------------|-----------------|
| Daily totals query | 200-500ms | 20-50ms | 5-10ms |
| Macro logging | ‚ùå Broken | ‚úÖ Works | ‚úÖ Works + Fast |
| Weekly aggregation | 1-2s | 200-400ms | 50-100ms |
| Database size | Baseline | +5% | +20% (normalized) |

---

## üîç DEBUGGING CHECKLIST

If macros still don't save after fix:
- [ ] Check browser console for errors
- [ ] Verify Supabase RLS policies allow inserts
- [ ] Check network tab for failed requests
- [ ] Verify user_id is correct
- [ ] Check Supabase logs for errors
- [ ] Ensure `revalidatePath('/')` is working
